{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T17:18:27.745378Z",
     "start_time": "2019-04-09T17:18:27.727083Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import contractions\n",
    "import csv\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import operator\n",
    "\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "from queue import PriorityQueue\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load & Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T16:37:02.245957Z",
     "start_time": "2019-04-09T16:37:01.929993Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "class Voc: # Word - Index Mapping\n",
    "    def __init__(self, name, version):\n",
    "        self.name = name\n",
    "        self.version = version\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"} \n",
    "        self.num_words = 3  # Count SOS, EOS, PAD\n",
    "        if version == \"word2vec\" or version=='word2vec_small':\n",
    "            from torchnlp.word_to_vector import GloVe\n",
    "            if version == \"word2vec\":\n",
    "                self.dim = 300 \n",
    "                self.glove = GloVe()\n",
    "                self.weights_matrix = np.zeros((10000,self.dim))\n",
    "            else:\n",
    "                self.dim = 100 \n",
    "                self.glove = GloVe(name='6B', dim=self.dim)\n",
    "            self.weights_matrix = np.zeros((10000,self.dim))\n",
    "            self.weights_matrix[0] = self.glove[str(PAD_token)]\n",
    "            self.weights_matrix[1] = self.glove[str(SOS_token)]\n",
    "            self.weights_matrix[2] = self.glove[str(EOS_token)]\n",
    "        elif version == \"bpemb\":\n",
    "            from bpemb import BPEmb\n",
    "            self.dim = 100\n",
    "            self.bpemb = BPEmb(lang=\"en\", dim=self.dim)\n",
    "            self.index2word = {PAD_token: self.bpemb.decode_ids([PAD_token]), \n",
    "                               SOS_token: self.bpemb.decode_ids([SOS_token]), \n",
    "                               EOS_token: self.bpemb.decode_ids([EOS_token])} \n",
    "            self.weights_matrix = self.bpemb.vectors.copy()\n",
    "    \n",
    "    def unicodeToAscii(self,s):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "        )\n",
    "    \n",
    "    def tokenizer(self,s):\n",
    "        s = self.unicodeToAscii(s.lower().strip())\n",
    "        s = contractions.fix(s)\n",
    "        s = re.sub(\n",
    "            r\"[\\*\\\"“”\\n\\\\…\\+\\-\\/\\=\\(\\)‘•:\\[\\]\\|’\\!;]\", \" \", \n",
    "            s)\n",
    "        s = re.sub(r\"[ ]+\", \" \", s)\n",
    "        s = re.sub(r\"\\!+\", \"!\", s)\n",
    "        s = re.sub(r\"\\,+\", \",\", s)\n",
    "        s = re.sub(r\"\\?+\", \"?\", s)\n",
    "\n",
    "        if self.version  == \"bpemb\":\n",
    "            return self.bpemb.encode(s)\n",
    "        else: \n",
    "            import spacy\n",
    "            NLP = spacy.load('en')\n",
    "            return [x.text for x in NLP.tokenizer(s) if x.text != \" \"]\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        words = self.tokenizer(sentence) if  type(sentence) == str else sentence\n",
    "        for word in words:\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "            if self.version in [\"word2vec\",\"word2vec_small\"]:\n",
    "                self.weights_matrix[self.num_words] = self.glove[word]\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "        keep_words = []\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        if version == \"bpemb\":\n",
    "            self.index2word = {PAD_token: bpemb.decode_ids([PAD_token]), \n",
    "                               SOS_token: bpemb.decode_ids([SOS_token]), \n",
    "                               EOS_token: bpemb.decode_ids([EOS_token])} \n",
    "        self.num_words = 3 # Count default tokens\n",
    "        if version in [\"word2vec\",\"word2vec_small\"]:\n",
    "            self.weights_matrix = np.zeros((len(keep_words),self.dim))\n",
    "            self.weights_matrix[0] = self.glove[PAD_token]\n",
    "            self.weights_matrix[1] = self.glove[SOS_token]\n",
    "            self.weights_matrix[2] = self.glove[EOS_token]                     \n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T17:30:24.421607Z",
     "start_time": "2019-04-09T17:30:24.210470Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Convert the json file to dataset of the format [post,[response,emotion],pos_emotion,res_emotion] for number of bucket\n",
    "def read_data(path,voc,max_size=None):\n",
    "    data_set = []\n",
    "    data = json.load(open(path,'r'))\n",
    "    counter = 0\n",
    "    size_max = 0\n",
    "    for pair in data:\n",
    "        post,emo1,emo2 = pair[0]\n",
    "        response,res_emo1,res_emo2 = pair[1][0]\n",
    "        post_word_list = voc.tokenizer(post)\n",
    "        res_word_list = voc.tokenizer(response)\n",
    "        if len(post_word_list) < MAX_LENGTH and len(res_word_list) < MAX_LENGTH:\n",
    "            voc.addSentence(post)\n",
    "            voc.addSentence(response)\n",
    "            counter += 1\n",
    "            if counter % 10000 == 0:\n",
    "                print(\"    reading data pair %d\" % counter)\n",
    "                print(post_word_list)\n",
    "                print(res_word_list)\n",
    "            data_set.append([post, response, int(emo1), int(res_emo1)])\n",
    "    return data_set\n",
    "\n",
    "def getword2index(word):\n",
    "    if word not in voc.word2index:\n",
    "        voc.addWord(word)\n",
    "    return voc.word2index[word]    \n",
    "def indexesFromSentence(voc, sentence):\n",
    "#     return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "    if voc.version == \"bpemb\":\n",
    "        return voc.bpemb.encode_ids(sentence) + [EOS_token]\n",
    "    return [getword2index(word) for word in voc.tokenizer(sentence)] + [EOS_token]\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if (type(token) == int and token == PAD_token) or (type(token)!=int and torch.equal(token,value)):\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "# Returns padded input sequence tensor and lengths\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)    \n",
    "    return padVar, lengths\n",
    "# Returns padded target sequence tensor, padding mask, and max target length\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])    \n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.ByteTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)    \n",
    "    return padVar, mask, max_target_len\n",
    "  \n",
    "# Returns all items for a given batch of pairs\n",
    "def batch2TrainData(voc, pair_batch):  \n",
    "    pair_batch.sort(key=lambda x: len(indexesFromSentence(voc,x[0])), reverse=True)\n",
    "#     pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch,emo_in,emo_out = [],[],[],[]\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "        pair[2] = 0 if pair[2] in [0,1,2] else pair[2]\n",
    "        pair[3] = 0 if pair[3] in [0,1,2] else pair[3]\n",
    "#         pair[2] = 0 if pair[2] in [1,2,3,5] else 1\n",
    "#         pair[3] = 0 if pair[2] in [1,2,3,5] else 1\n",
    "        emo_in.append(pair[2])\n",
    "        emo_out.append(pair[3])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len,torch.LongTensor(emo_in),torch.LongTensor(emo_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T17:22:35.318529Z",
     "start_time": "2019-04-09T17:22:35.305428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello . smith s resident .', 'hello . this is the operator . can i speak to mr . smith please ?', 0, 0]\n",
      "['he says he ll write a letter soon . he hopes we are all well . love jimmy .', 'is that all ? he doesn t say very much does he ?', 0, 0]\n"
     ]
    }
   ],
   "source": [
    "pair_batch = [random.choice(dev_set) for _ in range(small_batch_size)]\n",
    "print(pair_batch[0])\n",
    "pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "print(pair_batch[0])\n",
    "input_batch, output_batch,emo_in,emo_out = [],[],[],[]\n",
    "pair = pair_batch[0]\n",
    "input_batch.append(pair[0])\n",
    "output_batch.append(pair[1])\n",
    "emo_in.append(pair[2])\n",
    "emo_out.append(pair[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq Model\n",
    "**With emotion-embedding, internal memory, external memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder, Decoder and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T16:41:35.526344Z",
     "start_time": "2019-04-09T16:41:34.869787Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        #   because our input size is a word embedding with number of features == hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "        \n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # Convert word indexes to embeddings\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # Unpack padding\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        # Return output and final hidden state\n",
    "        return outputs, hidden\n",
    "# Luong attention layer\n",
    "class Attn(torch.nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Calculate the attention weights (energies) based on the given method\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        attn_energies = attn_energies.t()\n",
    "\n",
    "        # Return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "      \n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self,attn_model,embedding,emotion_embedding,hidden_size,output_size,n_layers=1,dropout=0.1,use_emb=False,use_imemory=False,use_ememory=False):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.use_emb = use_emb\n",
    "        self.use_imemory = use_imemory\n",
    "        self.use_ememory = use_ememory\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        self.emotion_embedding = emotion_embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat_1 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.concat_3 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        \n",
    "        # DIY layers\n",
    "        self.read_linear = nn.Linear(hidden_size*(self.n_layers+1),hidden_size)\n",
    "        self.write_linear = nn.Linear(hidden_size,hidden_size)\n",
    "        self.gru_concat = nn.Linear(hidden_size*2,hidden_size)\n",
    "        \n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "    def forward(self, input_step, emotion, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step (word) at a time\n",
    "        # Get embedding of current input word\n",
    "        embedded = self.embedding(input_step) # 1,64,300\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        \n",
    "        if self.use_emb and self.use_imemory:\n",
    "          \n",
    "            if emotion.size()!=embedded.size():\n",
    "                emotion = self.emotion_embedding(emotion).unsqueeze(dim=0) # mem_write\n",
    "            \n",
    "            _,tmp_size,_ = last_hidden.size()\n",
    "\n",
    "            read_gate = torch.sigmoid(self.read_linear(torch.cat([embedded,torch.reshape(last_hidden,(1,tmp_size,-1))],dim=2)))\n",
    "            mem_read = torch.mul(emotion,read_gate)\n",
    "            \n",
    "            gru_input = self.gru_concat(torch.cat([embedded,mem_read],dim=2))\n",
    "            rnn_output, hidden = self.gru(gru_input, last_hidden)\n",
    "            \n",
    "            write_gate = torch.sigmoid(self.write_linear(rnn_output))\n",
    "            emotion = torch.mul(write_gate,mem_read)\n",
    "          \n",
    "          \n",
    "          \n",
    "\n",
    "            attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "            # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
    "            context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "            # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "            rnn_output = rnn_output.squeeze(0)\n",
    "            context = context.squeeze(1)\n",
    "            concat_input = torch.cat((rnn_output, context), 1)\n",
    "            concat_output = torch.tanh(self.concat_3(concat_input))\n",
    "            # Predict next word using Luong eq. 6\n",
    "            output = self.out(concat_output)\n",
    "            output = F.softmax(output, dim=1)\n",
    "            return output,hidden,emotion\n",
    "        elif self.use_emb:\n",
    "            emotion_embedded = self.emotion_embedding(emotion).unsqueeze(dim=0)\n",
    "            embedded = self.concat_1(torch.cat([embedded,emotion_embedded],dim = 2))\n",
    "\n",
    "        # Forward through unidirectional GRU\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # Calculate attention weights from the current GRU output\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat_3(concat_input))\n",
    "        # Predict next word using Luong eq. 6\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        # Return output and final hidden state\n",
    "        return output,hidden,input_emotion      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Train and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T16:44:16.875239Z",
     "start_time": "2019-04-09T16:44:16.466286Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp,emotion,target,mask,decoder):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    if decoder.use_imemory:\n",
    "        emo_loss = torch.norm(emotion)\n",
    "        if math.isnan(emo_loss): loss+= emo_loss\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()\n",
    "MAX_LENGTH = 30\n",
    "def train(input_variable,input_emotion,lengths, target_variable, mask, max_target_len, encoder, decoder,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "#     Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "    input_emotion = input_emotion.to(device)\n",
    "    \n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "    # Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden,input_emotion = decoder(\n",
    "                decoder_input,input_emotion,decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output,input_emotion,target_variable[t], mask[t],decoder)\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden,input_emotion = decoder(\n",
    "                decoder_input,input_emotion,decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output,input_emotion,target_variable[t], mask[t],decoder)\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # Perform backpropatation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals\n",
    "\n",
    "def trainIters(model_name, voc, pairs, dev_pairs, encoder, decoder, encoder_optimizer, \n",
    "               decoder_optimizer, embedding,emo_embedding, encoder_n_layers, decoder_n_layers, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename=None):\n",
    "\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "    dev_batches = [batch2TrainData(voc, [random.choice(dev_pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        # Extract fields from batch\n",
    "        input_variable, lengths, target_variable, mask, max_target_len,emo_in,emo_out = training_batch\n",
    "        # Run a training iteration with batch\n",
    "        loss = train(input_variable,emo_out,lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "\n",
    "        print_loss += loss\n",
    "\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (iteration % save_every == 0):\n",
    "            dev_batch = dev_batches[iteration-1]\n",
    "            input_variable2, lengths2, target_variable2, mask2, max_target_len2,emo_in2,emo_out2 = dev_batch\n",
    "            \n",
    "            dev_loss = train(input_variable2,emo_out2, lengths2, target_variable2, mask2, max_target_len2, encoder,\n",
    "                     decoder,encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "            print(\"Iteration: {}; Dev loss: {:.4f}\".format(iteration, dev_loss))\n",
    "            directory = os.path.join(model_name, _corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict(),\n",
    "                'emo_embedding':emo_embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T16:44:18.044730Z",
     "start_time": "2019-04-09T16:44:17.865313Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class MyTopKDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder,k):\n",
    "        super(MyTopKDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.k = k\n",
    "    def forward(self, input_seq,target_emotion,input_length,num_output, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_output, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        \n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        emotion = torch.LongTensor([target_emotion]).to(device)\n",
    "        return beam_decode(decoder, emotion, decoder_hidden, encoder_output=encoder_output,topk=self.k,num_output = num_output,debug=True)     \n",
    "      \n",
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq,target_emotion, input_length, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "#         # Initialize decoder input with SOS_token\n",
    "#         decoder_input = torch.ones(1, 1, dtype=torch.long) * SOS_token\n",
    "#         # Initialize tensors to append decoded words to\n",
    "#         all_tokens = torch.zeros([0], dtype=torch.long)\n",
    "#         all_scores = torch.zeros([0])\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        # Iteratively decode one word token at a time\n",
    "        decoder_emotion = torch.LongTensor([target_emotion]).to(device)\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden, decoder_emotion = self.decoder(decoder_input,decoder_emotion, decoder_hidden, encoder_outputs)\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # Return collections of word tokens and scores\n",
    "        return all_tokens, all_scores\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T16:44:23.267384Z",
     "start_time": "2019-04-09T16:44:23.046889Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "beam_width = 15\n",
    "max_qsize = 1000\n",
    "max_past_word = 20\n",
    "similar_word_len = 2\n",
    "class BeamSearchNode(object):\n",
    "    def __init__(self, hiddenstate,hiddenemotion,previousNode, wordId, logProb, length, past=[]):\n",
    "        '''\n",
    "        :param hiddenstate:\n",
    "        :param previousNode:\n",
    "        :param wordId:\n",
    "        :param logProb:\n",
    "        :param length:\n",
    "        '''\n",
    "        self.h = hiddenstate\n",
    "        self.e = hiddenemotion\n",
    "        self.prevNode = previousNode\n",
    "        self.wordid = wordId\n",
    "        self.logp = logProb\n",
    "        self.leng = length\n",
    "        self.past = past\n",
    "        \n",
    "    def __lt__(self, other):      \n",
    "#         return self.logp < other.logp\n",
    "        return self.eval() < other.eval()\n",
    "    def eval(self, alpha=1.0):\n",
    "        reward = 0\n",
    "        # Add here a function for shaping a reward\n",
    "        return self.logp / float(self.leng - 1 + 1e-6) + alpha * reward\n",
    "def beam_decode(decoder,decoder_emotion,decoder_hidden,encoder_output,bram_width, num_output,debug=False):\n",
    "    sent_breaker = ['.','.','!','?',':']\n",
    "    # Start with the start of the sentence token\n",
    "    decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "    endnodes = []\n",
    "\n",
    "    # starting node -  hidden vector, previous node, word id, logp, length\n",
    "    node = BeamSearchNode(decoder_hidden, decoder_emotion, None, decoder_input, 0, 1)\n",
    "    nodes = PriorityQueue()\n",
    "    # start the queue\n",
    "    nodes.put((-node.eval(), node))\n",
    "    qsize = 1\n",
    "    # start beam search\n",
    "    while True:\n",
    "        # give up when decoding takes too long\n",
    "        if nodes.qsize() > max_qsize or nodes.empty(): break\n",
    "        # fetch the best node\n",
    "        score, n = nodes.get()\n",
    "        decoder_input = n.wordid\n",
    "        decoder_hidden = n.h\n",
    "        if n.wordid.item() == EOS_token and n.prevNode != None:\n",
    "            endnodes.append((score, n))\n",
    "            # if we reached maximum # of sentences required\n",
    "            if len(endnodes) >= num_output:break\n",
    "            else:\n",
    "                continue\n",
    "        # decode for one step using decoder\n",
    "        decoder_output,decoder_hidden,decoder_emotion = decoder(decoder_input,decoder_emotion,decoder_hidden,encoder_output)\n",
    "        # PUT HERE REAL BEAM SEARCH OF TOP\n",
    "        log_prob, indexes = torch.topk(decoder_output, beam_width)\n",
    "        \n",
    "        nextnodes = []\n",
    "        word_past = n.past.copy()\n",
    "        for new_k in range(beam_width):\n",
    "            decoded_t = indexes[0][new_k].view(1, -1)\n",
    "            eval_1 = voc.bpemb.decode_ids([decoded_t.item()]) in sent_breaker\n",
    "            eval_2 = decoded_t.item() in word_past\n",
    "            eval_3 = len(voc.bpemb.decode_ids([decoded_t.item()]))>similar_word_len\n",
    "            if (eval_1 and n.leng < max_past_word ) or (eval_2 and eval_3):\n",
    "                continue\n",
    "            else:   \n",
    "#                 print(\"current word is {}\".format(voc.bpemb.decode_ids([decoded_t.item()])))\n",
    "                log_p = log_prob[0][new_k].item()\n",
    "                word_past.append(decoded_t.item())\n",
    "                if len(word_past)>max_past_word:\n",
    "                    word_past = word_past[-max_past_word:]\n",
    "                node = BeamSearchNode(decoder_hidden,decoder_emotion, n,decoded_t,n.logp + log_p, n.leng + 1,word_past)\n",
    "                score = -node.eval()\n",
    "                nextnodes.append((score, node))\n",
    "        # put them into queue\n",
    "        for i in range(len(nextnodes)):\n",
    "            score, nn = nextnodes[i]\n",
    "            nodes.put((score, nn))\n",
    "\n",
    "    \n",
    "    # choose nbest paths, back trace them\n",
    "    if len(endnodes) == 0:\n",
    "        endnodes = [nodes.get() for _ in range(min(num_output,nodes.qsize()))]\n",
    "    utterances = []\n",
    "#     print(len(endnodes))\n",
    "    for score, n in sorted(endnodes, key=operator.itemgetter(0)):\n",
    "\n",
    "        utterance = []\n",
    "        utterance.append(n.wordid)\n",
    "        # back trace\n",
    "        while n.prevNode != None:\n",
    "            n = n.prevNode\n",
    "            utterance.append(n.wordid)\n",
    "        \n",
    "        utterance = utterance[::-1]\n",
    "        utterances.append(utterance)\n",
    "\n",
    "\n",
    "    return utterances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T17:14:05.950672Z",
     "start_time": "2019-04-09T17:14:05.853505Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def evaluateInput(encoder, decoder, searcher, voc, num_output=5, max_length=10):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input('> ')\n",
    "            # Check if it is quit case\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            # Normalize sentence\n",
    "#             input_sentence = normalizeString(input_sentence)\n",
    "            # words -> indexes\n",
    "            indexes_batch = [indexesFromSentence(voc, input_sentence)]\n",
    "            # Create lengths tensor\n",
    "            lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "            # Transpose dimensions of batch to match models' expectations\n",
    "            input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)            \n",
    "            # Use appropriate device\n",
    "            input_batch = input_batch.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            # Decode sentence with searcher\n",
    "            output = []\n",
    "            for e in range(num_emotion):\n",
    "                if type(searcher) == MyTopKDecoder:  \n",
    "                    tmp = searcher(input_batch,e,lengths, num_output, max_length)\n",
    "#                     res = tmp[e][:num_output]\n",
    "#                     print(tmp)\n",
    "                    for i in range(min(len(tmp),num_output)):\n",
    "                        if voc.version == \"bpemb\":\n",
    "                            padding = [0,1,2]\n",
    "                            words = [token.item() for token in tmp[i]]\n",
    "                            filtered = filter(lambda x: True if x not in padding else False, words)\n",
    "                            print('{}: '.format(int2emotion[e]), voc.bpemb.decode_ids(list(filtered)))\n",
    "                        else:\n",
    "                            decoded_words = [voc.index2word[token.item()] for token in tmp[i]]\n",
    "                            decoded_words[:] = [x for x in decoded_words if not (x == 'EOS' or x == 'PAD' or x == 'SOS')]\n",
    "                            print('{}: '.format(int2emotion[e]), ' '.join(decoded_words))\n",
    "                        \n",
    "                elif type(searcher) == GreedySearchDecoder:\n",
    "                    tokens, scores = searcher(input_batch,e,lengths, max_length)\n",
    "                    if voc.version == \"bpemb\":\n",
    "                        padding = [0,1,2]\n",
    "                        \n",
    "                        words = [token.item() for token in tokens]\n",
    "                        filtered = filter(lambda x: True if x not in padding else False, words)\n",
    "                        print('{}: '.format(int2emotion[e]), voc.bpemb.decode_ids(list(filtered)))\n",
    "                    else:\n",
    "                        decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "                        decoded_words[:] = [x for x in decoded_words if not (x == 'EOS' or x == 'PAD' or x == 'SOS')]\n",
    "                        print('{}: '.format(int2emotion[e]), ' '.join(decoded_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T17:19:50.086461Z",
     "start_time": "2019-04-09T17:18:58.181804Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400869/400869 [00:02<00:00, 162051.54B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading https://nlp.h-its.org/bpemb/en/en.wiki.bpe.vs10000.d100.w2v.bin.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3784656/3784656 [00:07<00:00, 531507.27B/s] \n",
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    reading data pair 10000\n",
      "['▁y', 'es', '▁i', '▁d', '▁like', '▁another', '▁cup', '▁of', '▁coffee', '▁afterwards', '▁.', '▁make', '▁it', '▁hot', '▁ple', 'ase', '▁.']\n",
      "['▁you', '▁ve', '▁got', '▁it', '▁sir', '▁.']\n",
      "    reading data pair 20000\n",
      "['▁there', '▁are', '▁hundreds', '▁and', '▁hundreds', '▁.', '▁english', '▁is', '▁particularly', '▁rich', '▁in', '▁id', 'i', 'om', 'atic', '▁express', 'ions', '▁.']\n",
      "['▁can', '▁you', '▁give', '▁us', '▁an', '▁example', '▁', '?']\n",
      "    reading data pair 30000\n",
      "['▁why', '▁did', '▁you', '▁lie', '▁to', '▁me', '▁in', '▁the', '▁em', 'ail', '▁', '?']\n",
      "['▁i', '▁didn', '▁t', '▁lie', '▁.', '▁you', '▁just', '▁didn', '▁t', '▁ask', '▁me', '▁my', '▁real', '▁name', '▁.']\n",
      "    reading data pair 10000\n",
      "['▁y', 'es', '▁i', '▁d', '▁like', '▁another', '▁cup', '▁of', '▁coffee', '▁afterwards', '▁.', '▁make', '▁it', '▁hot', '▁ple', 'ase', '▁.']\n",
      "['▁you', '▁ve', '▁got', '▁it', '▁sir', '▁.']\n",
      "    reading data pair 20000\n",
      "['▁there', '▁are', '▁hundreds', '▁and', '▁hundreds', '▁.', '▁english', '▁is', '▁particularly', '▁rich', '▁in', '▁id', 'i', 'om', 'atic', '▁express', 'ions', '▁.']\n",
      "['▁can', '▁you', '▁give', '▁us', '▁an', '▁example', '▁', '?']\n",
      "    reading data pair 30000\n",
      "['▁why', '▁did', '▁you', '▁lie', '▁to', '▁me', '▁in', '▁the', '▁em', 'ail', '▁', '?']\n",
      "['▁i', '▁didn', '▁t', '▁lie', '▁.', '▁you', '▁just', '▁didn', '▁t', '▁ask', '▁me', '▁my', '▁real', '▁name', '▁.']\n"
     ]
    }
   ],
   "source": [
    "_corpus_name = \"DailyDialogue\"\n",
    "_voc_name = \"bpemb\"\n",
    "\n",
    "train_path = os.path.join(_corpus_name,'train')\n",
    "dev_path = os.path.join(_corpus_name,'train')\n",
    "\n",
    "small_batch_size = 5\n",
    "MAX_LENGTH = 30  # Maximum sentence length to consider\n",
    "int2emotion = ['Anger','Happiness','Sadness','Surpise','Other']\n",
    "num_emotion = 5\n",
    "\n",
    "\n",
    "voc = Voc(_corpus_name,_voc_name) # Need to run the cell down below first\n",
    "train_set = read_data(train_path,voc)\n",
    "dev_set = read_data(dev_path,voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T17:45:46.048050Z",
     "start_time": "2019-04-09T17:45:45.957371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "model_name = 'ecm_model_imemory_bpemb_notfix'\n",
    "\n",
    "use_embedding =  True\n",
    "use_imemory = True\n",
    "use_ememory = False\n",
    "emb_learnable = True\n",
    "\n",
    "attn_model = 'dot'\n",
    "#attn_model = 'general'\n",
    "#attn_model = 'concat'\n",
    "\n",
    "hidden_size = 100 # must match pretrained word2vec embedding size!!!!\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "loadFilename = None\n",
    "checkpoint_iter = 2000\n",
    "# filepath = \"content/ecm_model_withimemory_bpemb/DailyDialogue/2-2_100/2000_checkpoint.tar\"\n",
    "# loadFilename = os.path.join('ecm_model_imemory_bpemb_notfix/DailyDialogue/2-2_100/2000_checkpoint.tar')\n",
    "\n",
    "# Load model if a loadFilename is provided\n",
    "if loadFilename:\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename)    \n",
    "    encoder_sd = checkpoint['en']\n",
    "    decoder_sd = checkpoint['de']\n",
    "    encoder_optimizer_sd = checkpoint['en_opt']\n",
    "    decoder_optimizer_sd = checkpoint['de_opt']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    voc.__dict__ = checkpoint['voc_dict']    \n",
    "print('Building encoder and decoder ...')\n",
    "\n",
    "# Initialize word embeddings\n",
    "wm = voc.weights_matrix if voc.version == \"bpemb\" else voc.weights_matrix[:voc.num_words]\n",
    "num_embeddings, embedding_dim = wm.shape\n",
    "embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "embedding.load_state_dict({'weight': torch.Tensor(wm)})\n",
    "emo_embedding = nn.Embedding(num_emotion,embedding_dim)\n",
    "\n",
    "\n",
    "if use_embedding and not emb_learnable:\n",
    "        embedding.weight.requires_grad = False\n",
    "\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = AttnDecoderRNN(attn_model, embedding, emo_embedding, hidden_size, voc.weights_matrix.shape[0], decoder_n_layers, \n",
    "                         dropout,use_emb=use_embedding, use_imemory=use_imemory, use_ememory=use_ememory)\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T17:41:59.204836Z",
     "start_time": "2019-04-09T17:41:59.200091Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.weights_matrix.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T17:23:56.018930Z",
     "start_time": "2019-04-09T17:23:34.746118Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers ...\n",
      "Starting Training!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-ab7b863b855e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m trainIters(model_name, voc, train_set, dev_set, encoder, decoder, encoder_optimizer, decoder_optimizer,\n\u001b[1;32m     25\u001b[0m            \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memo_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_n_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_n_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m            print_every, save_every, clip, _corpus_name, loadFilename)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-b11bff72efb1>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(model_name, voc, pairs, dev_pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, emo_embedding, encoder_n_layers, decoder_n_layers, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# Load batches for each iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n\u001b[0;32m---> 90\u001b[0;31m                       for _ in range(n_iteration)]\n\u001b[0m\u001b[1;32m     91\u001b[0m     dev_batches = [batch2TrainData(voc, [random.choice(dev_pairs) for _ in range(batch_size)])\n\u001b[1;32m     92\u001b[0m                       for _ in range(n_iteration)]\n",
      "\u001b[0;32m<ipython-input-13-b11bff72efb1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# Load batches for each iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n\u001b[0;32m---> 90\u001b[0;31m                       for _ in range(n_iteration)]\n\u001b[0m\u001b[1;32m     91\u001b[0m     dev_batches = [batch2TrainData(voc, [random.choice(dev_pairs) for _ in range(batch_size)])\n\u001b[1;32m     92\u001b[0m                       for _ in range(n_iteration)]\n",
      "\u001b[0;32m<ipython-input-6-55e7ee338684>\u001b[0m in \u001b[0;36mbatch2TrainData\u001b[0;34m(voc, pair_batch)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0memo_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_target_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_target_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memo_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memo_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-55e7ee338684>\u001b[0m in \u001b[0;36moutputVar\u001b[0;34m(l, voc)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Returns padded target sequence tensor, padding mask, and max target length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0moutputVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mindexes_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexesFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mmax_target_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindexes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexes_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mpadList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeroPadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexes_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-55e7ee338684>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Returns padded target sequence tensor, padding mask, and max target length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0moutputVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mindexes_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexesFromSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0mmax_target_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindexes\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexes_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mpadList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeroPadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexes_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-55e7ee338684>\u001b[0m in \u001b[0;36mindexesFromSentence\u001b[0;34m(voc, sentence)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#     return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"bpemb\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbpemb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mEOS_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetword2index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mEOS_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mzeroPadding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfillvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPAD_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/bpemb/bpemb.py\u001b[0m in \u001b[0;36mencode_ids\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mbyte\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mencoded\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \"\"\"\n\u001b[0;32m--> 225\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEncodeAsIds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     def encode_with_eos(\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/bpemb/bpemb.py\u001b[0m in \u001b[0;36m_encode\u001b[0;34m(self, texts, fn)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_preproc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_preproc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sentencepiece.py\u001b[0m in \u001b[0;36mEncodeAsIds\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mEncodeAsIds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_sentencepiece\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentencePieceProcessor_EncodeAsIds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mNBestEncodeAsPieces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbest_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 10000\n",
    "print_every = 5\n",
    "save_every = 100\n",
    "\n",
    "# Ensure dropout layers are in train mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# Run training iterations\n",
    "print(\"Starting Training!\")\n",
    "trainIters(model_name, voc, train_set, dev_set, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, emo_embedding, encoder_n_layers, decoder_n_layers, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, _corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Training and Train Iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T18:50:35.953836Z",
     "start_time": "2019-04-09T18:50:35.914902Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 2. Got 64 and 5 in dimension 1 at /Users/administrator/nightlies/pytorch-1.0.0/wheel_build_dirs/conda_3.6/conda/conda-bld/pytorch_1544137972173/work/aten/src/TH/generic/THTensorMoreMath.cpp:1333",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-fcef77ea9f86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0minput_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_target_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memo_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memo_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpair_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m loss = train(input_variable,emo_out,lengths, target_variable, mask, max_target_len, encoder,\n\u001b[0;32m----> 9\u001b[0;31m                      decoder, encoder_optimizer, decoder_optimizer, batch_size, clip)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-b11bff72efb1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_variable, input_emotion, lengths, target_variable, mask, max_target_len, encoder, decoder, encoder_optimizer, decoder_optimizer, batch_size, clip, max_length)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_target_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             decoder_output, decoder_hidden,input_emotion = decoder(\n\u001b[0;32m---> 48\u001b[0;31m                 \u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_emotion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             )\n\u001b[1;32m     50\u001b[0m             \u001b[0;31m# Teacher forcing: next input is current target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-69fe603e9ef4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_step, emotion, last_hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtmp_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlast_hidden\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mread_gate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtmp_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mmem_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mread_gate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 2. Got 64 and 5 in dimension 1 at /Users/administrator/nightlies/pytorch-1.0.0/wheel_build_dirs/conda_3.6/conda/conda-bld/pytorch_1544137972173/work/aten/src/TH/generic/THTensorMoreMath.cpp:1333"
     ]
    }
   ],
   "source": [
    "## TrainIter\n",
    "pairs = train_set\n",
    "dev_pairs = dev_set\n",
    "corpus_name = _corpus_name\n",
    "\n",
    "pair_batch = batch2TrainData(voc,[random.choice(dev_set) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len,emo_in,emo_out = pair_batch\n",
    "loss = train(input_variable,emo_out,lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, encoder_optimizer, decoder_optimizer, batch_size, clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T18:29:23.904886Z",
     "start_time": "2019-04-09T18:29:23.885510Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Train\n",
    "input_emotion = emo_out\n",
    "encoder_optimizer.zero_grad()\n",
    "decoder_optimizer.zero_grad()\n",
    "input_variable = input_variable.to(device)\n",
    "lengths = lengths.to(device)\n",
    "target_variable = target_variable.to(device)\n",
    "mask = mask.to(device)\n",
    "encoder_outputs, encoder_hidden = encoder(input_variable, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T18:32:53.087297Z",
     "start_time": "2019-04-09T18:32:53.069495Z"
    }
   },
   "outputs": [],
   "source": [
    "## Encoder\n",
    "embedded = embedding(input_variable)\n",
    "# Pack padded batch of sequences for RNN module\n",
    "packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, lengths)\n",
    "# Forward pass through GRU\n",
    "outputs, hidden = gru(packed,hidden)\n",
    "# Unpack padding\n",
    "outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "# Sum bidirectional GRU outputs\n",
    "# outputs = outputs[:, :, :hidden_size] + outputs[:, : ,hidden_size:]\n",
    "\n",
    "# decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "# decoder_input = decoder_input.to(device)\n",
    "# input_emotion = input_emotion.to(device)\n",
    "# decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "\n",
    "# decoder_output, decoder_hidden,input_emotion = decoder(\n",
    "#                 decoder_input,input_emotion,decoder_hidden, encoder_outputs\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T18:32:57.351541Z",
     "start_time": "2019-04-09T18:32:57.347011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 5, 100])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T18:28:12.493476Z",
     "start_time": "2019-04-09T18:28:12.488721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 5, 100]) torch.Size([18, 5, 0])\n"
     ]
    }
   ],
   "source": [
    "print(outputs[:, :, :hidden_size].size(),outputs[:, : ,hidden_size:].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T17:55:23.384906Z",
     "start_time": "2019-04-09T17:55:23.362029Z"
    }
   },
   "outputs": [],
   "source": [
    "n_layers = 2 \n",
    "output_size = num_embeddings\n",
    "gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "concat_1 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "concat_3 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "out = nn.Linear(hidden_size, output_size)\n",
    "read_linear = nn.Linear(hidden_size*(n_layers+1),hidden_size)\n",
    "write_linear = nn.Linear(hidden_size,hidden_size)\n",
    "gru_concat = nn.Linear(hidden_size*2,hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T17:56:08.625083Z",
     "start_time": "2019-04-09T17:56:08.601428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 100]) torch.Size([1, 5, 100]) 5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 2. Got 64 and 5 in dimension 1 at /Users/administrator/nightlies/pytorch-1.0.0/wheel_build_dirs/conda_3.6/conda/conda-bld/pytorch_1544137972173/work/aten/src/TH/generic/THTensorMoreMath.cpp:1333",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-6d1ee2b63bd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtmp_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mread_gate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtmp_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mmem_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mread_gate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 2. Got 64 and 5 in dimension 1 at /Users/administrator/nightlies/pytorch-1.0.0/wheel_build_dirs/conda_3.6/conda/conda-bld/pytorch_1544137972173/work/aten/src/TH/generic/THTensorMoreMath.cpp:1333"
     ]
    }
   ],
   "source": [
    "## Decoder\n",
    "# decoder = AttnDecoderRNN(attn_model,embedding,emo_embedding,hidden_size,num_embeddings,decoder_n_layers, \n",
    "#                          dropout,use_emb=use_embedding, use_imemory=use_imemory, use_ememory=use_ememory)\n",
    "\n",
    "embedded = embedding(decoder_input) # 1,64,300\n",
    "# emo_embedding = nn.Embedding(num_emotion,embedding_dim)\n",
    "emotion = emo_embedding(input_emotion).unsqueeze(dim=0) \n",
    "_,tmp_size,_ = decoder_hidden.size()\n",
    "print(embedded.size(),emotion.size(),tmp_size)\n",
    "\n",
    "read_gate = torch.sigmoid(read_linear(torch.cat([embedded,torch.reshape(decoder_hidden,(1,tmp_size,-1))],dim=2)))\n",
    "\n",
    "# mem_read = torch.mul(emotion,read_gate)\n",
    "# gru_input = gru_concat(torch.cat([embedded,mem_read],dim=2))\n",
    "# rnn_output, hidden = gru(gru_input, decoder_hidden)\n",
    "# write_gate = torch.sigmoid(write_linear(rnn_output))\n",
    "# emotion = torch.mul(write_gate,mem_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-09T17:57:48.524628Z",
     "start_time": "2019-04-09T17:57:48.520411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 100])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self,attn_model,embedding,emotion_embedding,hidden_size,output_size,n_layers=1,dropout=0.1,use_emb=False,use_imemory=False,use_ememory=False):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.use_emb = use_emb\n",
    "        self.use_imemory = use_imemory\n",
    "        self.use_ememory = use_ememory\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        self.emotion_embedding = emotion_embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat_1 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.concat_3 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        \n",
    "        # DIY layers\n",
    "        self.read_linear = nn.Linear(hidden_size*(self.n_layers+1),hidden_size)\n",
    "        self.write_linear = nn.Linear(hidden_size,hidden_size)\n",
    "        self.gru_concat = nn.Linear(hidden_size*2,hidden_size)\n",
    "        \n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "    def forward(self, input_step, emotion, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step (word) at a time\n",
    "        # Get embedding of current input word\n",
    "        embedded = self.embedding(input_step) # 1,64,300\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        \n",
    "        if self.use_emb and self.use_imemory:\n",
    "          \n",
    "            if emotion.size()!=embedded.size():\n",
    "                emotion = self.emotion_embedding(emotion).unsqueeze(dim=0) # mem_write\n",
    "            \n",
    "            _,tmp_size,_ = last_hidden.size()\n",
    "\n",
    "            read_gate = torch.sigmoid(self.read_linear(torch.cat([embedded,torch.reshape(last_hidden,(1,tmp_size,-1))],dim=2)))\n",
    "            mem_read = torch.mul(emotion,read_gate)\n",
    "            \n",
    "            gru_input = self.gru_concat(torch.cat([embedded,mem_read],dim=2))\n",
    "            rnn_output, hidden = self.gru(gru_input, last_hidden)\n",
    "            \n",
    "            write_gate = torch.sigmoid(self.write_linear(rnn_output))\n",
    "            emotion = torch.mul(write_gate,mem_read)\n",
    "          \n",
    "          \n",
    "          \n",
    "\n",
    "            attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "            # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
    "            context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "            # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "            rnn_output = rnn_output.squeeze(0)\n",
    "            context = context.squeeze(1)\n",
    "            concat_input = torch.cat((rnn_output, context), 1)\n",
    "            concat_output = torch.tanh(self.concat_3(concat_input))\n",
    "            # Predict next word using Luong eq. 6\n",
    "            output = self.out(concat_output)\n",
    "            output = F.softmax(output, dim=1)\n",
    "            return output,hidden,emotion\n",
    "        elif self.use_emb:\n",
    "            emotion_embedded = self.emotion_embedding(emotion).unsqueeze(dim=0)\n",
    "            embedded = self.concat_1(torch.cat([embedded,emotion_embedded],dim = 2))\n",
    "\n",
    "        # Forward through unidirectional GRU\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # Calculate attention weights from the current GRU output\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat_3(concat_input))\n",
    "        # Predict next word using Luong eq. 6\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        # Return output and final hidden state\n",
    "        return output,hidden,input_emotion      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
